<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <link rel="icon" type="image/x-icon" href="/favicon.ico">
  <link rel="apple-touch-icon" href="/apple-touch-icon.png">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Text processing in deep learning | Lucas’s Blog</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Text processing in deep learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="处理文本数据" />
<meta property="og:description" content="处理文本数据" />
<link rel="canonical" href="http://localhost:4000/archives/Text-processing-in-deep-learning.html" />
<meta property="og:url" content="http://localhost:4000/archives/Text-processing-in-deep-learning.html" />
<meta property="og:site_name" content="Lucas’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-09-09T00:00:00+08:00" />
<script type="application/ld+json">
{"headline":"Text processing in deep learning","dateModified":"2019-09-09T00:00:00+08:00","url":"http://localhost:4000/archives/Text-processing-in-deep-learning.html","datePublished":"2019-09-09T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/archives/Text-processing-in-deep-learning.html"},"description":"处理文本数据","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link href="https://fonts.googleapis.com/css?family=Domine|Asap|Noto+Sans+SC|Noto+Serif+SC&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Lucas's Blog" /></head><body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Lucas&#39;s Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/links/">Links</a><a class="page-link" href="/tags/">Tags</a><a class="page-link" href="/categories/">Categories</a><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <script type="text/javascript" async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Text processing in deep learning</h1>
    <p class="post-meta"><time class="dt-published" datetime="2019-09-09T00:00:00+08:00" itemprop="datePublished">Published: Sep 9, 2019</time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="处理文本数据">处理文本数据</h2>

<p>向量与标记相关联的方式有很多种，比较常用的有</p>
<ul>
  <li>one-hot编码(one-hot encoding)</li>
  <li>标记嵌入(token embedding)，通常只用于单词，也称词嵌入(word embedding)</li>
</ul>

<h3 id="one-hot-encoding">one-hot encoding</h3>
<p>以下假设每个单元都为一个单词。</p>

<p>one-hot encoding 将每个标记与一个唯一的整数索引相关联，然后将这个整数索引<script type="math/tex">i</script>转换成长度为<script type="math/tex">N</script>的binary向量，这个向量只有第<script type="math/tex">i</script>个元素是1，其余为0.</p>

<p><code class="highlighter-rouge">Keras</code>的内置函数可以完成对原始文本数据进行单词级或字符级的one-hot编码。这些函数还提供了许多特性，比如去除特殊字符、只考虑数据集中前<script type="math/tex">N</script>个最常见单词。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">samples</span> <span class="o">=</span> <span class="p">[</span><span class="s">'The cat sat on the mat.'</span><span class="p">,</span> <span class="s">'The dog ate my homework'</span><span class="p">]</span>

<span class="kn">from</span> <span class="nn">keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="c1"># [[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]] 
</span><span class="n">sequences</span> <span class="o">=</span>  <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="n">one_hot_results</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_matrix</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">'binary'</span><span class="p">)</span>

<span class="c1"># {'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}
</span><span class="n">word_index</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span>
</code></pre></div></div>

<h3 id="词嵌入">词嵌入</h3>

<p>使用密集向量(低维的浮点数向量)来表示。</p>

<p>获取词嵌入的两种方法：</p>

<ul>
  <li>在完成主任务的同时学习词嵌入</li>
  <li>预训练词嵌入：在其他任务上预计算好词嵌入，然后将其加载到模型中</li>
</ul>

<p>对第1种方法，<code class="highlighter-rouge">keras</code> 提供了<code class="highlighter-rouge">Embedding</code>层来学习词嵌入。<code class="highlighter-rouge">Embedding</code>层的作用可以理解为一种字典查找，输入是一个二维张量<code class="highlighter-rouge">(samples, seq_length)</code>，输出三维浮点数张量 <code class="highlighter-rouge">(samples, seq_length, embedding_dimensionality)</code>.</p>

<blockquote>
  <p>单词索引  -&gt; <code class="highlighter-rouge">Embedding</code>层 -&gt; 对应的词向量</p>
</blockquote>

<h3 id="词袋">词袋</h3>

<p>文本的一种表示方式，本质上是一个集合，其舍弃了文本中的语法结构。</p>

<h3 id="imdb-数据集">IMDB 数据集</h3>

<ol>
  <li><a href="http://ai.stanford.edu/~amaas/data/sentiment/">Large Movie Review Database</a></li>
  <li><a href="http://mng.bz/0tIo">Movie Review Database-2</a></li>
</ol>

<p>Tips: 解压数据集后得到结果：</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>data/aclImdb/
├── <span class="nb">test</span>
│   ├── neg
│   ├── pos
└── train
    ├── neg
    ├── pos
</code></pre></div></div>

<p>一次性读入数据可以采用<code class="highlighter-rouge">sklearn</code>内置函数处理，而不用手动写函数</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_files</span>
<span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">load_files</span><span class="p">(</span><span class="s">"data/aclImdb/train/"</span><span class="p">),</span> <span class="n">load_files</span><span class="p">(</span><span class="s">"data/aclImdb/test/"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="小结">小结</h2>

  </div>

  <p>写的不错，分享一下：</p>
  
  <a href="https://twitter.com/intent/tweet?text=Text processing in deep learning&url=http://localhost:4000/archives/Text-processing-in-deep-learning.html" title="Share on Twitter" rel="nofollow" target="_blank">Twitter</a>
  <a href="https://facebook.com/sharer.php?u=http://localhost:4000/archives/Text-processing-in-deep-learning.html" title="Share on Facebook" rel="nofollow" target="_blank">Facebook</a>
  <a href="https://plus.google.com/share?url=http://localhost:4000/archives/Text-processing-in-deep-learning.html" title="Share on Google+" rel="nofollow" target="_blank">Google+</a><a class="u-url" href="/archives/Text-processing-in-deep-learning.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Lucas&#39;s Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name"></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/ssrzz"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">ssrzz</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>I&#39;m a deep learning lover.</p>
      </div>
    </div>

  </div>

</footer><script src="/scripts/common.js?t=20191001191256"></script>

  

  </body>
</html>