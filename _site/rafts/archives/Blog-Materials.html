
<script type="text/javascript" async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting" style="padding: 0 15%">

    <header class="post-header">
      <h1 class="post-title p-name" itemprop="name headline">Blog Materials</h1>
      <p class="post-meta"><time class="dt-published" datetime="2019-11-20T00:00:00+08:00" itemprop="datePublished">Published: Nov 20, 2019</time></p>
    </header>
  
    <div class="post-content e-content" itemprop="articleBody">
      <p>Boosting 与 bagging 区别联系
自举汇聚法(bootstrap aggregating)，也称为Bagging ，是从原始数据集选择S次后得到S个新数据集的一种技术。</p>

<p>Boosting 是一种与 bagging很类似的技术，但在boosting中，不同的分类器是通过串行训练而获得的，每个新分类器都根据已经训练出来的分类器的性能来进行训练。boosting是通过集中关注被已有分类器错分的那些数据来获得新的分类器。 
Bagging中的分类器权重是相等的，而boosting并不相等，每个权重代表的是其对应分类器在上一轮迭代中的成功度。</p>

<p>Boosting代表 AdaBoost, XGBoost</p>

<h2 id="adaboost---自适应boosting">AdaBoost - 自适应Boosting</h2>
<h3 id="adaptive-boosting">Adaptive boosting</h3>
<p>运行过程: 训练数据中的每一个样本，并赋予其一个权重。首先在训练数据上训练出一个弱分类器并计算该分类器的错误率<script type="math/tex">\epsilon = \frac{N_{\text{wrong}}}{N_\text{all}}</script>，然后在同一数据集上再次训练弱分类器，在这次训练中，将会调整样本的权重，第一次分对的样本权重会降低，第一次分错的样本权重将会提高。</p>

<ol>
  <li>基于单层决策树构建弱分类器。</li>
</ol>

<p>单层决策树(decision stump，也称决策树桩)是一棵只有一个根结点，两个叶子结点的简单决策树。 它是AdaBoost中最流行(并不是唯一)的弱分类器，</p>

<h2 id="cart">CART</h2>
<p>CART, Classification And Regression Trees, 分类决策树。优点，可以对复杂和非线性的数据建模；缺点是，结果不易理解。</p>

<p>CART 采用二元切分来处理连续型变量，即每次把数据集切成两份，如果数据的某特征值大于切分所要求的值，那么这些数据就进入树的左子树，反之进入树的右子树。</p>

<p>如何度量连续型数值的不一致度？首先计算所有数据的均值，然后计算每条数据的值到均值的差值(绝对值或者平方值)。</p>

<h2 id="decision-trees">Decision Trees</h2>
<ul>
  <li>计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可能处理不相关特征数据</li>
  <li>可能会过拟合</li>
</ul>

<h3 id="id3">ID3</h3>
<p>Iternative Dichotomizer, the first of three Decision Tree implementations developed by Ross Quinlan (Quinlan, J. R. 1986.  Induction of Decision Trees. Mach. Learn. 1, 1 (Mar. 1986), 81-106.)</p>

<p>ID3的一般思路是：</p>
<ol>
  <li>测量集合数据的熵</li>
  <li>寻找最优方案(特征)划分数据集</li>
  <li>对子集进行递归划分直到子集中所有数据属于同一个分类，或者特征耗尽</li>
</ol>

<p>划分数据集的大原则是：<strong>将无序的数据变得更加有序</strong>。ID3使用<strong>信息增益</strong>(数据集划分前后信息发生的变化)的方法来划分。</p>

<p>要计算信息增益，我们需要一种度量集合信息的方式，比如香农熵(简称熵)。熵定义为信息的期望值，对于待分类的事物，符号<script type="math/tex">x_i</script>的信息定义为
<script type="math/tex">l(x_i) = - \text{log}_2p(x_i)</script>，其中<script type="math/tex">p(x_i)</script>为该分类的的概率。</p>

<p>由这些分类构成的集合的熵 <script type="math/tex">H = -\Sigma_{i=1}^n p(x_i) \text{log}_2 p(x_i)</script> 。 从物理意义上直观的讲，熵对应一个系统的混乱与不一致程度，熵越大，表明这个系统越混乱。
信息增益刻画的是：熵的减少或者数据无序度的减少。</p>

<h3 id="gini-impurity">Gini impurity</h3>
<p>TODO</p>

<h3 id="decision-tree-python-implementation">Decision Tree Python Implementation</h3>

<p><a href="/codes/decision_tree.py.txt">Deicision-Tree-ID3-Python3</a></p>

<p>ID3 的缺陷：</p>
<ol>
  <li>数据集不够大时，很容易过拟合</li>
  <li>每次只能考察一个特征来作决策</li>
  <li>无数处理(连续)的数值特征及缺失值</li>
</ol>

<h3 id="id3-vs-c45">ID3 VS. C4.5</h3>
<ol>
  <li>ID3 uses information gain whereas C4.5 uses gain ratio for splitting.</li>
  <li>ID3 每次划分分组时都会消耗特征，即划分数据分组之后特征数目会减少，而C4.5 &amp; CART并不总是消耗特征</li>
  <li>TODO</li>
</ol>

<p>C4.5 over ID3</p>
<ol>
  <li>accepts both continuous and discrete features</li>
  <li>handles incomplete data points;</li>
  <li>solves over-fitting problem by (very clever) bottom-up technique usually known as “pruning”;</li>
  <li>different weights can be applied the features that comprise the training data.</li>
</ol>


    </div>
  
    <hr>
<<<<<<< HEAD
    <p style="font-family: 'Times New Roman', Times, serif;">Great article, share it:</p>
    <a href="https://twitter.com/intent/tweet?text=Blog Materials&url=http://localhost:4000/archives/Blog-Materials.html" title="Share on Twitter" rel="nofollow" target="_blank"> <img src="/images/logos/twitterlogo.png" width="50px"> </a>
    <a href="https://facebook.com/sharer.php?u=http://localhost:4000/archives/Blog-Materials.html" title="Share on Facebook" rel="nofollow" target="_blank"> <img src="/images/logos/facebook-logo2.png" width="50px"> </a>
    <a href="http://service.weibo.com/share/share.php?url=http://localhost:4000/archives/Blog-Materials.html&appkey=&title=Blog Materials&pic=&ralateUid=&language=zh_cn" title="Share on weibo" rel="nofollow" target="_blank"> <img src="/images/logos/weibo-logo2.jpg" width="50px"> </a>
      
=======
    <p style="font-family: 'Times New Roman', Times, serif;" align='right'>Sharing is caring  
    <a href="https://twitter.com/intent/tweet?text=Blog Materials&url=http://localhost:4000/archives/Blog-Materials.html" title="Share on Twitter" rel="nofollow" target="_blank"> <img src="/images/logos/twitterlogo.png" width="50px"> </a>
    <a href="https://facebook.com/sharer/sharer.php?u=http://localhost:4000/archives/Blog-Materials.html" title="Share on Facebook" target="_blank"> <img src="/images/logos/facebook-logo2.png" width="50px"> </a>
    <a href="http://service.weibo.com/share/share.php?url=http://localhost:4000/archives/Blog-Materials.html&appkey=&title=Blog Materials&pic=&ralateUid=&language=zh_cn" title="Share on weibo" rel="nofollow" target="_blank"> <img src="/images/logos/weibo-logo2.jpg" width="50px"> </a>
    </p>
  
>>>>>>> dev<a class="u-url" href="/archives/Blog-Materials.html" hidden></a>
  </article>