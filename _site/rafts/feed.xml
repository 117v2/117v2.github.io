<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-11-26T13:56:52+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Not a Blog</title><author><name></name></author><entry><title type="html">Decision Trees</title><link href="http://localhost:4000/archives/Decision-Trees.html" rel="alternate" type="text/html" title="Decision Trees" /><published>2019-11-25T00:00:00+08:00</published><updated>2019-11-25T00:00:00+08:00</updated><id>http://localhost:4000/archives/Decision-Trees</id><content type="html" xml:base="http://localhost:4000/archives/Decision-Trees.html">&lt;h1 id=&quot;decision-trees&quot;&gt;Decision Trees&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可能处理不相关特征数据&lt;/li&gt;
  &lt;li&gt;可能会过拟合&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;id3&quot;&gt;ID3&lt;/h2&gt;
&lt;p&gt;Iternative Dichotomizer, the first of three Decision Tree implementations developed by Ross Quinlan (Quinlan, J. R. 1986.  Induction of Decision Trees. Mach. Learn. 1, 1 (Mar. 1986), 81-106.)&lt;/p&gt;

&lt;p&gt;ID3的一般思路是：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;测量集合数据的熵&lt;/li&gt;
  &lt;li&gt;寻找最优方案(特征)划分数据集&lt;/li&gt;
  &lt;li&gt;对子集进行递归划分直到子集中所有数据属于同一个分类，或者特征耗尽&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;划分数据集的大原则是：&lt;strong&gt;将无序的数据变得更加有序&lt;/strong&gt;。ID3使用&lt;strong&gt;信息增益&lt;/strong&gt;(数据集划分前后信息发生的变化)的方法来划分。&lt;/p&gt;

&lt;p&gt;要计算信息增益，我们需要一种度量集合信息的方式，比如香农熵(简称熵)。熵定义为信息的期望值，对于待分类的事物，符号&lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;的信息定义为
&lt;script type=&quot;math/tex&quot;&gt;l(x_i) = - \text{log}_2p(x_i)&lt;/script&gt;，其中&lt;script type=&quot;math/tex&quot;&gt;p(x_i)&lt;/script&gt;为该分类的的概率。&lt;/p&gt;

&lt;p&gt;由这些分类构成的集合的熵 &lt;script type=&quot;math/tex&quot;&gt;H = -\Sigma_{i=1}^n p(x_i) \text{log}_2 p(x_i)&lt;/script&gt; 。 从物理意义上直观的讲，熵对应一个系统的混乱与不一致程度，熵越大，表明这个系统越混乱。
信息增益刻画的是：熵的减少或者数据无序度的减少。&lt;/p&gt;

&lt;h3 id=&quot;gini-impurity&quot;&gt;Gini impurity&lt;/h3&gt;
&lt;p&gt;TODO&lt;/p&gt;

&lt;h3 id=&quot;decision-tree-python-implementation&quot;&gt;Decision Tree Python Implementation&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;/codes/decision_tree.py.txt&quot;&gt;Deicision-Tree-ID3-Python3&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ID3 的缺陷：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;数据集不够大时，很容易过拟合&lt;/li&gt;
  &lt;li&gt;每次只能考察一个特征来作决策&lt;/li&gt;
  &lt;li&gt;无数处理(连续)的数值特征及缺失值&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;id3-vs-c45&quot;&gt;ID3 VS. C4.5&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;ID3 uses information gain whereas C4.5 uses gain ratio for splitting.&lt;/li&gt;
  &lt;li&gt;ID3 每次划分分组时都会消耗特征，即划分数据分组之后特征数目会减少，而C4.5 &amp;amp; CART并不总是消耗特征&lt;/li&gt;
  &lt;li&gt;TODO&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;C4.5 over ID3&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;accepts both continuous and discrete features&lt;/li&gt;
  &lt;li&gt;handles incomplete data points;&lt;/li&gt;
  &lt;li&gt;solves over-fitting problem by (very clever) bottom-up technique usually known as “pruning”;&lt;/li&gt;
  &lt;li&gt;different weights can be applied the features that comprise the training data.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;cart&quot;&gt;CART&lt;/h1&gt;
&lt;p&gt;CART, Classification And Regression Trees, 分类决策树。优点，可以对复杂和非线性的数据建模；缺点是，结果不易理解。&lt;/p&gt;

&lt;p&gt;CART 采用二元切分来处理连续型变量，即每次把数据集切成两份，如果数据的某特征值大于切分所要求的值，那么这些数据就进入树的左子树，反之进入树的右子树。&lt;/p&gt;

&lt;p&gt;如何度量连续型数值的不一致度？首先计算所有数据的均值，然后计算每条数据的值到均值的差值(绝对值或者平方值)。&lt;/p&gt;</content><author><name></name></author><category term="decision tree" /><category term="supervised learning" /><summary type="html">Decision Trees 计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可能处理不相关特征数据 可能会过拟合</summary></entry><entry><title type="html">Blog Materials</title><link href="http://localhost:4000/archives/Blog-Materials.html" rel="alternate" type="text/html" title="Blog Materials" /><published>2019-11-20T00:00:00+08:00</published><updated>2019-11-20T00:00:00+08:00</updated><id>http://localhost:4000/archives/Blog-Materials</id><content type="html" xml:base="http://localhost:4000/archives/Blog-Materials.html">&lt;p&gt;Boosting 与 bagging 区别联系
自举汇聚法(bootstrap aggregating)，也称为Bagging ，是从原始数据集选择S次后得到S个新数据集的一种技术。&lt;/p&gt;

&lt;p&gt;Boosting 是一种与 bagging很类似的技术，但在boosting中，不同的分类器是通过串行训练而获得的，每个新分类器都根据已经训练出来的分类器的性能来进行训练。boosting是通过集中关注被已有分类器错分的那些数据来获得新的分类器。 
Bagging中的分类器权重是相等的，而boosting并不相等，每个权重代表的是其对应分类器在上一轮迭代中的成功度。&lt;/p&gt;

&lt;p&gt;Boosting代表 AdaBoost, XGBoost&lt;/p&gt;

&lt;h2 id=&quot;adaboost---自适应boosting&quot;&gt;AdaBoost - 自适应Boosting&lt;/h2&gt;
&lt;h3 id=&quot;adaptive-boosting&quot;&gt;Adaptive boosting&lt;/h3&gt;
&lt;p&gt;运行过程: 训练数据中的每一个样本，并赋予其一个权重。首先在训练数据上训练出一个弱分类器并计算该分类器的错误率&lt;script type=&quot;math/tex&quot;&gt;\epsilon = \frac{N_{\text{wrong}}}{N_\text{all}}&lt;/script&gt;，然后在同一数据集上再次训练弱分类器，在这次训练中，将会调整样本的权重，第一次分对的样本权重会降低，第一次分错的样本权重将会提高。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;基于单层决策树构建弱分类器。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;单层决策树(decision stump，也称决策树桩)是一棵只有一个根结点，两个叶子结点的简单决策树。 它是AdaBoost中最流行(并不是唯一)的弱分类器，&lt;/p&gt;

&lt;h2 id=&quot;cart&quot;&gt;CART&lt;/h2&gt;
&lt;p&gt;CART, Classification And Regression Trees, 分类决策树。优点，可以对复杂和非线性的数据建模；缺点是，结果不易理解。&lt;/p&gt;

&lt;p&gt;CART 采用二元切分来处理连续型变量，即每次把数据集切成两份，如果数据的某特征值大于切分所要求的值，那么这些数据就进入树的左子树，反之进入树的右子树。&lt;/p&gt;

&lt;p&gt;如何度量连续型数值的不一致度？首先计算所有数据的均值，然后计算每条数据的值到均值的差值(绝对值或者平方值)。&lt;/p&gt;

&lt;h2 id=&quot;decision-trees&quot;&gt;Decision Trees&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可能处理不相关特征数据&lt;/li&gt;
  &lt;li&gt;可能会过拟合&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;id3&quot;&gt;ID3&lt;/h3&gt;
&lt;p&gt;Iternative Dichotomizer, the first of three Decision Tree implementations developed by Ross Quinlan (Quinlan, J. R. 1986.  Induction of Decision Trees. Mach. Learn. 1, 1 (Mar. 1986), 81-106.)&lt;/p&gt;

&lt;p&gt;ID3的一般思路是：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;测量集合数据的熵&lt;/li&gt;
  &lt;li&gt;寻找最优方案(特征)划分数据集&lt;/li&gt;
  &lt;li&gt;对子集进行递归划分直到子集中所有数据属于同一个分类，或者特征耗尽&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;划分数据集的大原则是：&lt;strong&gt;将无序的数据变得更加有序&lt;/strong&gt;。ID3使用&lt;strong&gt;信息增益&lt;/strong&gt;(数据集划分前后信息发生的变化)的方法来划分。&lt;/p&gt;

&lt;p&gt;要计算信息增益，我们需要一种度量集合信息的方式，比如香农熵(简称熵)。熵定义为信息的期望值，对于待分类的事物，符号&lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;的信息定义为
&lt;script type=&quot;math/tex&quot;&gt;l(x_i) = - \text{log}_2p(x_i)&lt;/script&gt;，其中&lt;script type=&quot;math/tex&quot;&gt;p(x_i)&lt;/script&gt;为该分类的的概率。&lt;/p&gt;

&lt;p&gt;由这些分类构成的集合的熵 &lt;script type=&quot;math/tex&quot;&gt;H = -\Sigma_{i=1}^n p(x_i) \text{log}_2 p(x_i)&lt;/script&gt; 。 从物理意义上直观的讲，熵对应一个系统的混乱与不一致程度，熵越大，表明这个系统越混乱。
信息增益刻画的是：熵的减少或者数据无序度的减少。&lt;/p&gt;

&lt;h3 id=&quot;gini-impurity&quot;&gt;Gini impurity&lt;/h3&gt;
&lt;p&gt;TODO&lt;/p&gt;

&lt;h3 id=&quot;decision-tree-python-implementation&quot;&gt;Decision Tree Python Implementation&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;/codes/decision_tree.py.txt&quot;&gt;Deicision-Tree-ID3-Python3&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ID3 的缺陷：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;数据集不够大时，很容易过拟合&lt;/li&gt;
  &lt;li&gt;每次只能考察一个特征来作决策&lt;/li&gt;
  &lt;li&gt;无数处理(连续)的数值特征及缺失值&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;id3-vs-c45&quot;&gt;ID3 VS. C4.5&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;ID3 uses information gain whereas C4.5 uses gain ratio for splitting.&lt;/li&gt;
  &lt;li&gt;ID3 每次划分分组时都会消耗特征，即划分数据分组之后特征数目会减少，而C4.5 &amp;amp; CART并不总是消耗特征&lt;/li&gt;
  &lt;li&gt;TODO&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;C4.5 over ID3&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;accepts both continuous and discrete features&lt;/li&gt;
  &lt;li&gt;handles incomplete data points;&lt;/li&gt;
  &lt;li&gt;solves over-fitting problem by (very clever) bottom-up technique usually known as “pruning”;&lt;/li&gt;
  &lt;li&gt;different weights can be applied the features that comprise the training data.&lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><category term="machine learning" /><category term="blog" /><summary type="html">Boosting 与 bagging 区别联系 自举汇聚法(bootstrap aggregating)，也称为Bagging ，是从原始数据集选择S次后得到S个新数据集的一种技术。</summary></entry><entry><title type="html">Batch Normalization</title><link href="http://localhost:4000/archives/Batch-Normalization.html" rel="alternate" type="text/html" title="Batch Normalization" /><published>2019-11-09T00:00:00+08:00</published><updated>2019-11-09T00:00:00+08:00</updated><id>http://localhost:4000/archives/Batch-Normalization</id><content type="html" xml:base="http://localhost:4000/archives/Batch-Normalization.html">&lt;h2 id=&quot;what-is-batch-normalization&quot;&gt;What is batch normalization&lt;/h2&gt;

&lt;h2 id=&quot;why-we-use-it&quot;&gt;Why we use it&lt;/h2&gt;
&lt;p&gt;Generally speaking, when training a neural network, we want to normalize or standardize our data in pre-processing step to put all the data in the same scale. &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;z = (x-m) / s&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Without normalization, relatively large inputs can cascade down through the layers in the network, which may cause imbalance gradients, which may cause the famous &lt;strong&gt;exploding gradient problem&lt;/strong&gt;.
Besides, non-normalized data can significantly decrease our training speed.&lt;/p&gt;

&lt;p&gt;But this is not the end of this &lt;em&gt;normalization story&lt;/em&gt;, once the normalized input data were fed into the network, weights of the model were updated during each epoch via SGD. If one of those weights ends up becoming drastically larger than other weights, then the output from its corresponding neuron might be extremely large and this imbalance will again continue to cascade through the network causing instability.&lt;/p&gt;

&lt;p&gt;This is where BN comes into play. With BN, we have normalized data coming in and normalized data within the model.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;BN reduces the amount by what the hidden unit values shift around (covariance shift)&lt;/li&gt;
  &lt;li&gt;BN allows each layer of a network to learn by itself a little bit more independently of other layers.&lt;/li&gt;
  &lt;li&gt;It reduces overfitting because it has a slight regularization effects. Similar to dropout, it adds some noise to each hidden layer’s activations.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-to-implement-it-in-our-algorithm&quot;&gt;How to implement it in our algorithm&lt;/h2&gt;
&lt;p&gt;Batch normalization normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation. 
This process occurs on per batch basis, hence the name &lt;strong&gt;batch norm&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;alternatives&quot;&gt;Alternatives&lt;/h2&gt;

&lt;h2 id=&quot;materials&quot;&gt;Materials&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=nUUqwaxLnWs&quot;&gt;Andrew Ng explains BN&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="machine learning" /><summary type="html">What is batch normalization</summary></entry><entry><title type="html">Machine Learning Softwares</title><link href="http://localhost:4000/archives/Machine-Learning-Softwares.html" rel="alternate" type="text/html" title="Machine Learning Softwares" /><published>2019-11-01T00:00:00+08:00</published><updated>2019-11-01T00:00:00+08:00</updated><id>http://localhost:4000/archives/Machine-Learning-Softwares</id><content type="html" xml:base="http://localhost:4000/archives/Machine-Learning-Softwares.html">&lt;h1 id=&quot;tensorflow-20&quot;&gt;TensorFlow 2.0&lt;/h1&gt;

&lt;h2 id=&quot;tfkeras-vs-keras&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tf.keras&lt;/code&gt; V.S. &lt;code class=&quot;highlighter-rouge&quot;&gt;Keras&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano.&lt;/p&gt;

&lt;p&gt;To understand the relationship between &lt;code class=&quot;highlighter-rouge&quot;&gt;Keras&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.keras&lt;/code&gt;, first, we have to clarify the complicated, intertwined relationship between &lt;code class=&quot;highlighter-rouge&quot;&gt;Keras&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;TensorFlow&lt;/code&gt;.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;2015.03.27, &lt;a href=&quot;https://fchollet.com/&quot;&gt;Francois Chollet&lt;/a&gt;(who is also the author of Deep Learning with Python) committed and released the first version of Keras to his GitHub to facilitate his own research and experiments.&lt;/li&gt;
  &lt;li&gt;Due to its easy-to-use API and the explosion of deep learning popularity, many developers, programmers, and machine learning practitioners flocked to Keras&lt;/li&gt;
  &lt;li&gt;Keras’ default backend was Theano (until v1.1.10)&lt;/li&gt;
  &lt;li&gt;Google released TensorFlow on November 9, 2015, Keras started supporting TensorFlow as a backend&lt;/li&gt;
  &lt;li&gt;Eventually, TensorFlow became the most popular backend, Keras v1.1.0 switched to TensorFlow as its default backend&lt;/li&gt;
  &lt;li&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.keras&lt;/code&gt; submodule was introduced in TensorFlow v1.10.0, the first step in integrating &lt;code class=&quot;highlighter-rouge&quot;&gt;Keras&lt;/code&gt; directly within the TensorFlow package itself.&lt;/li&gt;
  &lt;li&gt;Keras v2.3.0 was released on September 17, 2019. This is the final release of Keras that will support backend other than TensorFlow. Bugs present in multi-backend Keras will only be fixed until April 2020.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To summary, &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.keras&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Keras&lt;/code&gt; are two separated different modules. &lt;code class=&quot;highlighter-rouge&quot;&gt;Keras&lt;/code&gt; is a high-level API of TensorFlow, and &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.keras&lt;/code&gt; is a submodule of TensorFlow.
It is recommended to use &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.keras&lt;/code&gt; for future projects as the Keras package will only support bug fixes.&lt;/p&gt;

&lt;h2 id=&quot;how-to-update-to-tensorflow-20&quot;&gt;How to update to &lt;code class=&quot;highlighter-rouge&quot;&gt;TensorFlow 2.0&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;First of all, a virtual environment is strongly recommended to avoid potential package conflicts.&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;virtualenv &lt;span class=&quot;nt&quot;&gt;--system-site-packages&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; python3 myenv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;By running the above command, a virtual environment &lt;code class=&quot;highlighter-rouge&quot;&gt;myenv&lt;/code&gt; is created.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;--system-site-packages&lt;/code&gt; allows the projects within the virtual environment &lt;code class=&quot;highlighter-rouge&quot;&gt;myenv&lt;/code&gt; access the global site-packages. The default setting does not allow this access.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;-p python3&lt;/code&gt; is used to set the Python interpreter.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;myenv&lt;/code&gt; is the name of the virtual environment we created&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; /myenv/bin/activate
pip &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--upgrade&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tensorflow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;2.0.0-rc1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above command installs a 2.0.0-rc1 CPU-only version.&lt;/p&gt;

&lt;p&gt;To choose the appropriate TensorFlow version, visit &lt;a href=&quot;https://www.tensorflow.org/install/pip&quot;&gt;https://www.tensorflow.org/install/pip&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Alternative TensorFlow packages:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tensorflow==2.0.0-rc1&lt;/code&gt; Preview TF 2.0 RC build for CPU-only (recommended).&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tensorflow-gpu==2.0.0-rc1&lt;/code&gt; Preview TF 2.0 RC build with GPU support&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tensorflow&lt;/code&gt; Latest stable release for CPU-only.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tensorflow-gpu&lt;/code&gt; Latest stable release with GPU support.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tf-nightly&lt;/code&gt; Preview nightly build for CPU-only.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tf-nightly-gpu&lt;/code&gt; Preview nightly build with GPU support.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;test-the-installation&quot;&gt;Test the installation&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;import tensorflow as tf
print(tf.__version__)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;tensorflow-20-colab&quot;&gt;TensorFlow 2.0 Colab&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://colab.research.google.com&quot;&gt;Google Colab&lt;/a&gt; is promoting TF 2.0 (current version is still TF 1.5, Nov 14, 2019), if you want to use TF2.0 on Colab, you can manually install it :&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;err&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pip&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensorflow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gpu&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# or tensorflow-gpu==2.0.0-rc1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Note: you will have to install one of those packages with GPU support, otherwise there is no GPU acceleration even if you set the runtime type to GPU mode.&lt;/p&gt;

&lt;h1 id=&quot;keras&quot;&gt;Keras&lt;/h1&gt;
&lt;h2 id=&quot;callbacks-&quot;&gt;Callbacks ?&lt;/h2&gt;
&lt;p&gt;A callback provides a set of functions to be applied at given stages of the training procedure.&lt;/p&gt;

&lt;h3 id=&quot;earlystopping&quot;&gt;EarlyStopping&lt;/h3&gt;
&lt;p&gt;Stop training when a monitored quantity has stopped improving.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;callbacks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;callbacks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EarlyStopping&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;monitor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'val_loss'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;min_delta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;patience&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'auto'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;baseline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;restore_best_weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Arguments:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;monitor&lt;/code&gt;: quantity to be monitored&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;min_delta&lt;/code&gt;: minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;patience&lt;/code&gt;: number of epochs that produced the monitored quantity with no improvement after which training will be stopped. Validation quantities may not be produced for every epoch, if the validation frequency (model.fit(validation_freq=5)) is greater than one.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;baseline&lt;/code&gt;: Baseline value for the monitored quantity to reach. Training will stop if the model doesn’t show improvement over the baseline.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;restore_best_weights&lt;/code&gt;: whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;mode&lt;/code&gt;: one of {auto, min, max}. In &lt;code class=&quot;highlighter-rouge&quot;&gt;min&lt;/code&gt; mode, training will stop when the quantity monitored has stopped decreasing; in &lt;code class=&quot;highlighter-rouge&quot;&gt;max&lt;/code&gt; mode it will stop when the quantity monitored has stopped increasing; in &lt;code class=&quot;highlighter-rouge&quot;&gt;auto&lt;/code&gt; mode, the direction is automatically inferred from the name of the monitored quantity.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;reducelronplateau&quot;&gt;ReduceLROnPlateau&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Reduce learning rate&lt;/strong&gt; when a metric has stopped improving. For example, if &lt;code class=&quot;highlighter-rouge&quot;&gt;val_loss&lt;/code&gt; stayed unreduced in 10 epochs, the learning rate is reduced by 90%, i.e., new_lr = lr * factor.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;reduce_lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;callbacks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;callbacks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReduceLROnPlateau&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;monitor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'val_loss'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;factor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;patience&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'auto'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;min_delta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cooldown&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;min_lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;callbacks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;sklearn&quot;&gt;Sklearn&lt;/h2&gt;
&lt;h3 id=&quot;labelencoder&quot;&gt;LabelEncoder&lt;/h3&gt;
&lt;p&gt;Encode labels with value between 0 and n_classes - 1.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preprocessing&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;le&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preprocessing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LabelEncoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;le&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;apple&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;pen&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;apple&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;applepen&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# [0, 2, 0, 1]
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;le&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classes_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# ['apple', 'pen', 'applepen']
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;labelbinarizer&quot;&gt;LabelBinarizer&lt;/h3&gt;
&lt;p&gt;Very similar to &lt;code class=&quot;highlighter-rouge&quot;&gt;LabelEncoder&lt;/code&gt;, but creating a label indicator matrix, instead an array, from a list of multi-class labels.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;lb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preprocessing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LabelBinarizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lb&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'female'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'male'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'others'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'female'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt; 
&lt;span class=&quot;c1&quot;&gt;# [[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0]]
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;onehotencoder&quot;&gt;OneHotEncoder&lt;/h3&gt;
&lt;p&gt;Encode categorical integer features as a one-hot numeric array. The input to this transformer must be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;enc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OneHotEncoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;handle_unknown&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ignore'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'female'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'male'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'others'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;enc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# [[1. 0. 0.], [0. 1. 0.], [0. 0. 1.]]
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><category term="machine learning" /><category term="keras" /><category term="tensorflow" /><summary type="html">TensorFlow 2.0</summary></entry><entry><title type="html">Set up a domain-based Email with Yandex</title><link href="http://localhost:4000/archives/Set-up-a-domain-based-Email-with-Yandex.html" rel="alternate" type="text/html" title="Set up a domain-based Email with Yandex" /><published>2019-10-31T00:00:00+08:00</published><updated>2019-10-31T00:00:00+08:00</updated><id>http://localhost:4000/archives/Set-up-a-domain-based-Email-with-Yandex</id><content type="html" xml:base="http://localhost:4000/archives/Set-up-a-domain-based-Email-with-Yandex.html">&lt;p&gt;搭建个人域名邮箱有很多好处，自定义任意前缀，无需繁琐的注册过程，更不用担心注册信息泄露而成为大数据中的一员。&lt;/p&gt;

&lt;p&gt;域名邮箱搭建有多种方式，我们推荐使用&lt;a href=&quot;https://www.yandex.com&quot;&gt;Yandex&lt;/a&gt;(俄罗斯互联网巨头，旗下搜索引擎在本土市场占有率超60%)的服务来完成这项任务。推荐原因包括但不限于：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;注册相比自行搭建步骤简单&lt;/li&gt;
  &lt;li&gt;由Yandex做担保，稳定可靠，无需自行维护，且比一般小众邮箱更为安全&lt;/li&gt;
  &lt;li&gt;可自定义 1000 个邮箱(前缀)，每个用户10G容量，多开邮箱账号无需门槛&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一言以蔽之，快速、方便、省心。&lt;/p&gt;

&lt;h2 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;个人域名 (E.g., &lt;a href=&quot;https://www.name.com/referral/388121&quot;&gt;name.com&lt;/a&gt;上$1.99的&lt;code class=&quot;highlighter-rouge&quot;&gt;xyz&lt;/code&gt;域名)&lt;/li&gt;
  &lt;li&gt;Yandex 邮箱 (注册入口 &lt;a href=&quot;https://mail.yandex.com&quot;&gt;mail.yandex.com&lt;/a&gt;)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;假设个人域名为 &lt;code class=&quot;highlighter-rouge&quot;&gt;flyingkiwi.me&lt;/code&gt;。&lt;/p&gt;

&lt;h2 id=&quot;流程&quot;&gt;流程&lt;/h2&gt;
&lt;h3 id=&quot;1-注册yandex-mail&quot;&gt;1. 注册Yandex Mail&lt;/h3&gt;

&lt;p&gt;注册入口 &lt;a href=&quot;https://passport.yandex.com/registration&quot;&gt;passport.yandex.com&lt;/a&gt;。注册过程中会要求提供个人手机号码，如果不想提供，也可以通过设置安全问题来完成注册。&lt;/p&gt;

&lt;p&gt;注：刚注册完的邮箱在24小时内尽量不要发送邮件，有可能被Yandex认定为spammer.&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center&quot; src=&quot;/images/2019/passport.yandex.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2-注册域名邮箱&quot;&gt;2. 注册域名邮箱&lt;/h3&gt;
&lt;p&gt;个人域名邮箱申请入口 &lt;a href=&quot;https://connect.yandex.com/pdd/&quot;&gt;Connect.Yandex&lt;/a&gt;。
&lt;img class=&quot;center&quot; src=&quot;/images/2019/connect.yandex.png&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;填入 &lt;code class=&quot;highlighter-rouge&quot;&gt;flyingkiwi.me&lt;/code&gt;进行注册。之后需要确认域名所有权，并将域名委托至 Yandex 服务器。&lt;/p&gt;

&lt;p&gt;确认所有权主要通过以下4种方式 ：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Upload a file to the site directory&lt;/li&gt;
  &lt;li&gt;Add a CNAME record&lt;/li&gt;
  &lt;li&gt;Change the contact address through the registrar&lt;/li&gt;
  &lt;li&gt;Delegate a domain to Yandex servers&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;很多人推荐第3种方法，即设置DNS record，即在DNS控制面板处添加TXT记录&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;TXT yandex-verification 87cal27lacla22c2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;但实际上第一种方式更快速一些，即直接在服务器(VPS)根目录创建相应文件(&lt;a href=&quot;https://yandex.com/support/domain/setting/confirm.html#way1&quot;&gt;visit for more details&lt;/a&gt;)。基本上在3分钟之内就能被Yandex验证成功。&lt;/p&gt;

&lt;h3 id=&quot;3-配置mx-spf-and-dkim&quot;&gt;3. 配置MX, SPF and DKIM&lt;/h3&gt;
&lt;p&gt;Question : Why ?&lt;/p&gt;

&lt;p&gt;Answer: 邮件服务器解析、降低个人域名邮件被标记为垃圾邮件的风险。&lt;/p&gt;

&lt;h4 id=&quot;配置mx&quot;&gt;配置MX&lt;/h4&gt;
&lt;p&gt;在DNS管理平台上添加 MX 记录，优先级填写 10 
&lt;img class=&quot;center&quot; src=&quot;/images/2019/mx.yandex.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;spf-dkim&quot;&gt;SPF, DKIM&lt;/h4&gt;
&lt;p&gt;
在DNS管理平台上添加 TXT 记录 (&lt;a href=&quot;https://yandex.com/support/domain/set-mail/spf.html&quot;&gt;VISIT for more details&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Name: @  Value: v = spf1 redirect = _spf.yandex.net&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;DKIM signature is used to verify that whether a message really came from the supposed sender.
To do so, we need to create a TXT record for the domain &lt;code class=&quot;highlighter-rouge&quot;&gt;flyingkiwi.me&lt;/code&gt; with a public key signature, which can be generated from &lt;a href=&quot;https://connect.yandex.com/portal/admin/customization/mail&quot;&gt;HERE&lt;/a&gt;. 
For more detail, please visit &lt;a href=&quot;https://yandex.com/support/domain/set-mail/dkim.html&quot;&gt;THIS PAGE&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If all the previous steps are strictly followed, we should by now have our own domain-based Mail system. 
Add a new user is simple, just visit &lt;a href=&quot;https://connect.yandex.com/portal/admin/departments/1&quot;&gt;PROTAL.Yandex&lt;/a&gt; and hit &lt;code class=&quot;highlighter-rouge&quot;&gt;add a person&lt;/code&gt;, the rest is straightforward.&lt;/p&gt;

&lt;h2 id=&quot;config-domain-based-email-on-iphone&quot;&gt;Config domain-based email on iPhone&lt;/h2&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Passwords &amp;amp; Accounts&lt;/code&gt; ➡ &lt;code class=&quot;highlighter-rouge&quot;&gt;Add Account&lt;/code&gt; ➡ &lt;code class=&quot;highlighter-rouge&quot;&gt;Other&lt;/code&gt; ➡ &lt;code class=&quot;highlighter-rouge&quot;&gt;Add Mail Account&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;And then type in &lt;code class=&quot;highlighter-rouge&quot;&gt;i@flyingkiwi.me&lt;/code&gt;(your first domain-based email) and password and go next. Other infos and configurations are shown in the following image.&lt;/p&gt;

&lt;p&gt;Note that, the port for SMTP server &lt;code class=&quot;highlighter-rouge&quot;&gt;smtp.yandex.com&lt;/code&gt; is 465, while the default port given by iPhone is 587.&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center&quot; src=&quot;/images/2019/iphone.yandex.png&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;POP3: pop.yandex.com 995
SMTP smtp.yandex.com 465
IMAP imap.yandex.com 993
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;a-dns-configuration-example&quot;&gt;A DNS configuration example&lt;/h2&gt;

&lt;p&gt;&lt;img class=&quot;center&quot; src=&quot;/images/2019/summary.yandex.png&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><category term="Mail" /><category term="Yandex" /><summary type="html">搭建个人域名邮箱有很多好处，自定义任意前缀，无需繁琐的注册过程，更不用担心注册信息泄露而成为大数据中的一员。</summary></entry><entry><title type="html">Make command-line scripts easier with Argparse</title><link href="http://localhost:4000/archives/Make-command-line-scripts-easier-with-Argparse.html" rel="alternate" type="text/html" title="Make command-line scripts easier with Argparse" /><published>2019-10-27T00:00:00+08:00</published><updated>2019-10-27T00:00:00+08:00</updated><id>http://localhost:4000/archives/Make-command-line-scripts-easier-with-Argparse</id><content type="html" xml:base="http://localhost:4000/archives/Make-command-line-scripts-easier-with-Argparse.html">&lt;p&gt;When we execute a Python script with command-line, to pass arguments to the script, we usually adopt a strategy like:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;endswith&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;@gmail&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;do_something&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;    
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This is fine when the number of arguments is few, say, less than three. 
But when there are dozens of options to choose, codes following the aforementioned style become cumbersome, ugly and hard to debug or to refine whenever it is necessary. 
Lucky for us, with this &lt;a href=&quot;https://docs.python.org/3/library/argparse.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;argparse&lt;/code&gt;&lt;/a&gt; module, passing and parsing arguments can be easy and elegant.&lt;/p&gt;

&lt;p&gt;This &lt;code class=&quot;highlighter-rouge&quot;&gt;argparse&lt;/code&gt; module is the “recommended command-line parsing module in the Python standard library” and makes it easy to write user-friendly command-line interfaces.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;argparse&lt;/code&gt; can figure out how to parse arguments out of &lt;code class=&quot;highlighter-rouge&quot;&gt;sys.argv&lt;/code&gt; so we do not have to define it manually. It can also automatically generates help and usage messages and issues errors when users give the program invalid arguments.&lt;/p&gt;

&lt;p&gt;Great, that is exactly what we need in our scripts. 
Now, step 1: embed the module into our script by writting:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;argparse&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# and creating an object
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argparse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ArgumentParser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;examples-and-explanations&quot;&gt;Examples and Explanations&lt;/h1&gt;

&lt;h2 id=&quot;add-argument&quot;&gt;Add argument&lt;/h2&gt;
&lt;p&gt;Call the &lt;code class=&quot;highlighter-rouge&quot;&gt;add_argument()&lt;/code&gt; method to add program argument, the calls tell the object &lt;code class=&quot;highlighter-rouge&quot;&gt;parser&lt;/code&gt; how to take the strings on the command line and turn them into objects. E.g.,&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_argument&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;-d&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;--delete&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;help&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;remove invalid account from&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;And by running &lt;code class=&quot;highlighter-rouge&quot;&gt;py prog.py&lt;/code&gt;, the code returns&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;usage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prog&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DELETE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optional&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arguments&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;help&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;show&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;this&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;help&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;message&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;exit&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DELETE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delete&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DELETE&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;remove&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;invalid&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;account&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The help for this program will display&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;prog.py&lt;/code&gt; as the program name (regardless of where the program was invoked from)&lt;/li&gt;
  &lt;li&gt;A new arg &lt;code class=&quot;highlighter-rouge&quot;&gt;-d&lt;/code&gt; we just added&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you desire to display something other than the default &lt;code class=&quot;highlighter-rouge&quot;&gt;prog.py&lt;/code&gt;, you can simply pass a name to the &lt;code class=&quot;highlighter-rouge&quot;&gt;prog=&lt;/code&gt; arg.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argparse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ArgumentParser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'prog=simple_script'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;epilog&quot;&gt;Epilog&lt;/h3&gt;
&lt;p&gt;To display additional description of the program after the description of the arguments. Extra text can be specified using the &lt;code class=&quot;highlighter-rouge&quot;&gt;epilog=&lt;/code&gt; argument to ArgumentParser.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argparse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ArgumentParser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'prog=simple_script'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epilog&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Do go visit google.com if you want to know more&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;formatter_class&quot;&gt;formatter_class&lt;/h3&gt;
&lt;p&gt;Classes &lt;code class=&quot;highlighter-rouge&quot;&gt;class argparse.RawDescriptionHelpFormatter, class argparse.RawTextHelpFormatter&lt;/code&gt; help specifying an alternate formatting style. 
For example, to display description in multiple line:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;textwrap&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argparse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ArgumentParser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
     &lt;span class=&quot;n&quot;&gt;prog&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'PROG'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;n&quot;&gt;formatter_class&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argparse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RawDescriptionHelpFormatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;n&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;textwrap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dedent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'''&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;         Please do not mess up this text!
         --------------------------------
             1. information line 1
             2. information line 2
             ...
             998. information line 998
         '''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Produces:
&lt;img class=&quot;center&quot; src=&quot;/images/2019/python-argparse-1.png&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;add_help&quot;&gt;add_help&lt;/h3&gt;
&lt;p&gt;By default, ArgumentParser objects add an option which simply displays the parser’s help message. For example, consider following code:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;argparse&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argparse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ArgumentParser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_argument&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'--foo'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;help&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'foo help'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parse_args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><category term="Python" /><category term="Script" /><summary type="html">When we execute a Python script with command-line, to pass arguments to the script, we usually adopt a strategy like:</summary></entry><entry><title type="html">Materials on Machine Learning</title><link href="http://localhost:4000/archives/Materials-on-Machine-Learning.html" rel="alternate" type="text/html" title="Materials on Machine Learning" /><published>2019-10-20T00:00:00+08:00</published><updated>2019-10-20T00:00:00+08:00</updated><id>http://localhost:4000/archives/Materials-on-Machine-Learning</id><content type="html" xml:base="http://localhost:4000/archives/Materials-on-Machine-Learning.html">&lt;h2 id=&quot;image-classification&quot;&gt;Image Classification&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1502.01852v1.pdf&quot;&gt;Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;documents&quot;&gt;Documents&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.tensorflow.org/guide/keras/overview&quot;&gt;tf.keras document&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;materials&quot;&gt;Materials&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.thepythoncode.com/article/using-speech-recognition-to-convert-speech-to-text-python?fbclid=IwAR0UPx7WQupkEKeFUWejJwspQop8ZRAaRmDUvM6b0H6n13boExIETwBrb7U&quot;&gt;How to convert speech to text&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="machine learning" /><category term="papers" /><summary type="html">Image Classification Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</summary></entry><entry><title type="html">Gradient Descent</title><link href="http://localhost:4000/archives/Gradient-Descent.html" rel="alternate" type="text/html" title="Gradient Descent" /><published>2019-10-20T00:00:00+08:00</published><updated>2019-10-20T00:00:00+08:00</updated><id>http://localhost:4000/archives/Gradient-Descent</id><content type="html" xml:base="http://localhost:4000/archives/Gradient-Descent.html">&lt;h2 id=&quot;梯度gradient&quot;&gt;梯度(Gradient)&lt;/h2&gt;
&lt;p&gt;梯度是张量运算的导数，是导数这一概念向多元函数导数的推广。&lt;/p&gt;

&lt;p&gt;GD的优势：简单、有效，对于凸函数(convex)来说，GD总能很快找到最小值。 相应的，对于非凸函数(non-convex)，GD可能会陷入到一个局部最小值，而无法收敛到全局最小值。&lt;/p&gt;

&lt;p&gt;为了使网络收敛，改进方案: &lt;strong&gt;改变学习速率&lt;/strong&gt; 、 &lt;strong&gt;use momentum&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;rmsprop-optimizer&quot;&gt;RMSprop Optimizer&lt;/h2&gt;

&lt;h2 id=&quot;sgd&quot;&gt;SGD&lt;/h2&gt;
&lt;p&gt;缺点：如果函数的形状非均向(anisotropic)，比如呈延伸状，那么寻找最小值的路径将非常低效。其根本原因在于，梯度的方向并没有指向最小值的方向。&lt;/p&gt;

&lt;h2 id=&quot;小批量随机梯度下降mini-batch-stochastic-gradient-descent&quot;&gt;小批量随机梯度下降(mini-batch stochastic gradient descent)&lt;/h2&gt;
&lt;p&gt;思想：从样本中抽取训练样本&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;和对应目标，然后在&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;上运行网络。&lt;/p&gt;</content><author><name></name></author><category term="Macgine Learning" /><category term="gd" /><summary type="html">梯度(Gradient) 梯度是张量运算的导数，是导数这一概念向多元函数导数的推广。</summary></entry><entry><title type="html">Basic Math in Machine Learning</title><link href="http://localhost:4000/archives/Basic-Math-in-Machine-Learning.html" rel="alternate" type="text/html" title="Basic Math in Machine Learning" /><published>2019-10-14T00:00:00+08:00</published><updated>2019-10-14T00:00:00+08:00</updated><id>http://localhost:4000/archives/Basic-Math-in-Machine-Learning</id><content type="html" xml:base="http://localhost:4000/archives/Basic-Math-in-Machine-Learning.html">&lt;h1 id=&quot;随机变量及分布&quot;&gt;随机变量及分布&lt;/h1&gt;

&lt;h2 id=&quot;正态分布&quot;&gt;正态分布&lt;/h2&gt;

&lt;p&gt;若随机变量&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;的密度函数为&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x) = \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}}, -\infty \le x \le \infty&lt;/script&gt;

&lt;p&gt;则称 &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; 服从正态分布，记作 &lt;script type=&quot;math/tex&quot;&gt;X \sim N(\mu, \sigma^2)&lt;/script&gt;。其分布函数&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F(x) = \frac{1}{\sqrt{2\pi}\sigma} \int_{- \infty}^x  e^{-\frac{(t-\mu)^2}{2\sigma^2}} dt&lt;/script&gt;

&lt;p&gt;正态分布由两个参数决定 &lt;script type=&quot;math/tex&quot;&gt;\mu, \sigma&lt;/script&gt;：&lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt;决定密度函数的位置，称为位置参数； &lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt;决定函数的尺度，称为尺度函数。&lt;/p&gt;

&lt;p&gt;特别的，当&lt;script type=&quot;math/tex&quot;&gt;\mu=0, \sigma=1&lt;/script&gt;的正态分布&lt;script type=&quot;math/tex&quot;&gt;N(0, 1)&lt;/script&gt;为标准正态分布。&lt;/p&gt;

&lt;h3 id=&quot;一般正态分布的标准化&quot;&gt;一般正态分布的标准化&lt;/h3&gt;

&lt;p&gt;定理：若&lt;script type=&quot;math/tex&quot;&gt;X \sim N(\mu, \sigma^2)&lt;/script&gt;，则&lt;script type=&quot;math/tex&quot;&gt;U = (X - \mu) / \sigma \sim N(0, 1)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;这个定理表明一般正态分布都可以通过一个线性变换化成标准正态分布。&lt;/p&gt;

&lt;h3 id=&quot;正态分布期望与方差&quot;&gt;正态分布期望与方差&lt;/h3&gt;
&lt;p&gt;标准正态分布&lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt;的期望为 &lt;script type=&quot;math/tex&quot;&gt;E(U) = 1/\sqrt{2\pi} \int_{- \infty}^{\infty} u e^{-\frac{u^2}{2}} du&lt;/script&gt;，可以看到被积函数是一个奇函数(i.e., &lt;script type=&quot;math/tex&quot;&gt;\forall x \in \mathcal{D}, f(-x) = - f(x)&lt;/script&gt;)，因此积分值为0，即 &lt;script type=&quot;math/tex&quot;&gt;E(U) = 0&lt;/script&gt;。&lt;/p&gt;

&lt;p&gt;对于一般分布 &lt;script type=&quot;math/tex&quot;&gt;X = \mu + \sigma U&lt;/script&gt;，由期望的线性性得到： &lt;script type=&quot;math/tex&quot;&gt;E(X) = \mu + \sigma \times 0 = \mu&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;标准分布的方差&lt;script type=&quot;math/tex&quot;&gt;\text{Var}(U) = 1&lt;/script&gt;(证明略)，由方差的性质可得一般正态分布的方差 &lt;script type=&quot;math/tex&quot;&gt;\text{Var}(X) = \text{Var}(\sigma U + \mu) = \sigma^2 \text{Var}(U) = \sigma^2&lt;/script&gt;&lt;/p&gt;</content><author><name></name></author><category term="Macgine Learning" /><category term="Math" /><summary type="html">随机变量及分布</summary></entry><entry><title type="html">Learn Deep Learning</title><link href="http://localhost:4000/archives/Learn-Deep-Learning.html" rel="alternate" type="text/html" title="Learn Deep Learning" /><published>2019-09-25T00:00:00+08:00</published><updated>2019-09-25T00:00:00+08:00</updated><id>http://localhost:4000/archives/Learn-Deep-Learning</id><content type="html" xml:base="http://localhost:4000/archives/Learn-Deep-Learning.html">&lt;p&gt;20世纪40~60年代，控制论(cybernetics)。随着生物学习理论的发展与第一个模型的实现(如感知机1958)，能实现单个神经元的训练。&lt;/p&gt;

&lt;!--break--&gt;

&lt;h1 id=&quot;历史&quot;&gt;历史&lt;/h1&gt;

&lt;h2 id=&quot;三次浪潮&quot;&gt;三次浪潮&lt;/h2&gt;
&lt;p&gt;20世纪40~60年代，控制论(cybernetics)。随着生物学习理论的发展与第一个模型的实现(如感知机1958)，能实现单个神经元的训练。&lt;/p&gt;

&lt;h3 id=&quot;联结主义&quot;&gt;联结主义&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;20世纪80~90年代(1980~1995)，联结主义(connectionism)，其中心思想是，当网络将大量简单的计算单元连接在一起时可能实现智能行为。&lt;/li&gt;
  &lt;li&gt;联结主义是在认知科学的背景下出现的，20世纪80年代初期，大多数认知科学家研究符号推理模型，但很难解释大脑如何真正使用神经元实现推理功能。&lt;/li&gt;
  &lt;li&gt;成就
    &lt;ul&gt;
      &lt;li&gt;反向传播在训练具有内部表示的深度神经网络中的成功使用，以及反向传播算法的普及&lt;/li&gt;
      &lt;li&gt;LSTM(long short-term memory)的引入(Hochreiter &amp;amp; Schmidhuber,1997)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;尽管这个阶段提出的很多算法在今天都表现的很好，神经网络研究的浪潮还是在20世纪90年代中期逐渐衰退，总结下来，至少归因于以下几点:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;计算机软硬件基础设施发展滞后，以当时计算机硬件的水准，很难训练出足够水平的神经网络。&lt;/li&gt;
  &lt;li&gt;基于神经网络的创业公司野心勃勃但不切实际，令投资者失望&lt;/li&gt;
  &lt;li&gt;机器学习的其他领域，比如核方法(Boser et al. 1992)和图模型(Jordan, 1998)取得进步，并在很多重要任务上实现了很好的效果。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;2006~now，深度学习的复兴&lt;/p&gt;

&lt;h2 id=&quot;deep-learning-vs-ai&quot;&gt;Deep Learning v.s. AI&lt;/h2&gt;

&lt;p&gt;许多AI任务可以通过&lt;strong&gt;提取一个合适特征，然后将这些特征提供给简单的机器学习算法&lt;/strong&gt;来解决问题。 但提取哪些特征是一个难题，解决这个问题的途径之一是&lt;strong&gt;使用机器学习来发掘表示本身&lt;/strong&gt;，这种方法称为：表示学习(representation learning)。&lt;/p&gt;

&lt;p&gt;典型例子：自编码器(autoencoder)。学习到的表示往往比手动设计的表示表现的要好，且只需极少人工干预。&lt;/p&gt;

&lt;p&gt;表示学习的一个困难在于：多个变差因素同时影响着我们能够观察到的每一个数据，从原始数据是抽取高层次、抽象的特征非常困难。深度学习通过其他简单的表示来表达复杂表示，这解决了表示学习的核心问题。
典型例子：前馈深度网络。&lt;/p&gt;

&lt;p&gt;总：DL是ML的一种，是一种能够使用计算机系统从数据和经验中得到提升的技术。&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt; 
    &lt;img src=&quot;/images/dl.vs.ai.png&quot; width=&quot;500px&quot; /&gt;
&lt;/p&gt;

&lt;!-- Math --&gt;
&lt;h1 id=&quot;basic-math&quot;&gt;Basic Math&lt;/h1&gt;

&lt;h2 id=&quot;线性相关与生成子空间&quot;&gt;线性相关与生成子空间&lt;/h2&gt;
&lt;p align=&quot;center&quot;&gt;  $$ Ax = b  $$ &lt;/p&gt;
&lt;p&gt;其中 &lt;script type=&quot;math/tex&quot;&gt;A \in \mathrm{R}^{m \times n}&lt;/script&gt; matrix, &lt;script type=&quot;math/tex&quot;&gt;b \in \mathrm{R}^n&lt;/script&gt; vector。对于上面的方程组来说，要么不存在解，要么存在唯一解或者无穷解，不可能存在大于1但小于无穷个解的情况 (不然，两个解的线性组合 &lt;script type=&quot;math/tex&quot;&gt;\alpha x + (1-\alpha)y&lt;/script&gt;也是方程组的解)。&lt;/p&gt;

&lt;h3 id=&quot;方程在每一点存在解的必要条件-n-geq-m&quot;&gt;方程在每一点存在解的必要条件 &lt;script type=&quot;math/tex&quot;&gt;n \geq m&lt;/script&gt;&lt;/h3&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;Ax = \Sigma_{i\in[1,n]} x_i A_{:, i} = \Sigma_i c_i v^{(i)}&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;v^{(i)}&lt;/script&gt; 是&lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;的列向量。 判定以上方程组是否存在解，即判定&lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt;是否在&lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;的生成子空间中。 这个特殊的生成子空间称为&lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;的&lt;strong&gt;列空间&lt;/strong&gt;或&lt;strong&gt;值域(range)&lt;/strong&gt; .&lt;/p&gt;

&lt;p&gt;因为 &lt;script type=&quot;math/tex&quot;&gt;b \in \mathrm{R^m}&lt;/script&gt;，如果 &lt;script type=&quot;math/tex&quot;&gt;\mathrm{R}^m&lt;/script&gt; 中一个点不在 &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; 的列空间中，那该点对应的 &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; 没有解，因此 &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; 至少有 &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; 列，即 &lt;script type=&quot;math/tex&quot;&gt;n \geq m&lt;/script&gt;。 举例，&lt;script type=&quot;math/tex&quot;&gt;m = 3, n = 2&lt;/script&gt;，那么无论 &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; 如何变化，它只能将 &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; 映射到 &lt;script type=&quot;math/tex&quot;&gt;\mathrm{R}^3&lt;/script&gt;的一个平面，只有当 &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; 处于这个平面时， 方程才有解。&lt;/p&gt;

&lt;h3 id=&quot;存在解的充分条件&quot;&gt;存在解的充分条件&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;n \geq m&lt;/script&gt; 并不能保证方程一定存在解，因为列向量可能&lt;strong&gt;线性相关&lt;/strong&gt;，即某一个向量可能表示为其他一组向量的线性组合。要使其列空间涵盖整个 &lt;script type=&quot;math/tex&quot;&gt;\mathrm{R}^m&lt;/script&gt;， 需要满足什么条件 ？&lt;/p&gt;

&lt;p&gt;Ans: 矩阵必须包含&lt;strong&gt;至少一组&lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt;个线性无关&lt;/strong&gt;的向量。&lt;/p&gt;

&lt;!-- &gt; 方程 $$Ax=b$$ 对每一个$$b$$都有解的充分必要条件：向量集恰好有$$m$$个线性不相关的向量，而非至少有$$m$$个。 二者还有有区别的。因为至少有$$m$$个包含了 --&gt;

&lt;p&gt;但要使矩阵可逆，需要保证对每一个 &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; 至多只有一个解，因此要保证矩阵至多只有 &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; 个列向量，即必须是一个方阵(square)，且所有列向量线性无关。这样的矩阵称为 &lt;strong&gt;奇异矩阵&lt;/strong&gt;。&lt;/p&gt;

&lt;h3 id=&quot;特征分解&quot;&gt;特征分解&lt;/h3&gt;
&lt;p&gt;矩阵 &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; 的&lt;strong&gt;特征向量(eigenvector)&lt;/strong&gt;是指与 &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; 相乘后相当于对该向量进行缩放的非零向量 &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt;:  &lt;script type=&quot;math/tex&quot;&gt;Av = \lambda v&lt;/script&gt; ，
其中标题 &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; 称为这个特征向量对应的&lt;strong&gt;特征值(eigenvalue)&lt;/strong&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如果 &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; 是 &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; 的特征向量，那么任意缩放后的向量 &lt;script type=&quot;math/tex&quot;&gt;sv(s \in \mathrm{R}, s \neq 0)&lt;/script&gt; 也是 &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; 的特征向量&lt;/li&gt;
  &lt;li&gt;假设 &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; 有 &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; 个线性无关的特征向量  &lt;script type=&quot;math/tex&quot;&gt;\{v_1, ..., v_n\}&lt;/script&gt; ，对应的特征值  &lt;script type=&quot;math/tex&quot;&gt;\{\lambda_1, ..., \lambda_n\}&lt;/script&gt; 。将这些特征向量连接成一个矩阵  &lt;script type=&quot;math/tex&quot;&gt;V = \{v^{(1)}, ..., v^{(n)}\}&lt;/script&gt; ，那么有 &lt;script type=&quot;math/tex&quot;&gt;AV = V \text{diag}(\lambda) ( \text{ where }\lambda = [\lambda_1, ..., \lambda_n ]^T)&lt;/script&gt;，进一步地，&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;  $$A = V \text{diag}(\lambda) V^{-1}$$  &lt;/p&gt;
&lt;p&gt;其中&lt;/p&gt;

&lt;h3 id=&quot;奇异值分解&quot;&gt;奇异值分解&lt;/h3&gt;
&lt;p&gt;将矩阵分解为奇异向量(singular vector)和奇异值(singular value)的方法称为奇异值分解(singular value decomposition SVD).
每个实数矩阵都有一个SVD，但不一定有特征分解，e.g., 非方阵的矩阵没有特征分解。&lt;/p&gt;

&lt;!-- ## 行列式
### 主成分分析(principle component analysis PCA)
* Goal ?
 --&gt;

&lt;h1 id=&quot;概率与信息论&quot;&gt;概率与信息论&lt;/h1&gt;
&lt;p&gt;概率率是&lt;strong&gt;表示不确定性声明的数学框架&lt;/strong&gt;。它提供了量化不确定性的方法，也提供也用于推导新的不确定性声明的公理 。&lt;/p&gt;

&lt;p&gt;对于AI系统有两个方面的用途：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;概率法则告诉我们AI系统如何推理，据此我们可以设计一些算法或者估算由概率论导出来的表达式&lt;/li&gt;
  &lt;li&gt;可以用概率与统计从理论上分析我们提出的AI系统的行为&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;不确定性的来源：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;被建模系统内在的随机性&lt;/li&gt;
  &lt;li&gt;不完全观测&lt;/li&gt;
  &lt;li&gt;不完全建模, e.g., 用离散化的空间建模连续空间&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;概率分布&quot;&gt;概率分布&lt;/h2&gt;
&lt;p&gt;Probability distribution 描述了随机变量或者一簇随机变量在每一个可能取到的状态的可能性大小。&lt;/p&gt;

&lt;h3 id=&quot;离散型变量&quot;&gt;离散型变量&lt;/h3&gt;
&lt;p&gt;可以&lt;strong&gt;概率质量函数(probability mass function PMF)&lt;/strong&gt;描述。举例&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Bernoulli分布，单个二值随机变量分布：  &lt;script type=&quot;math/tex&quot;&gt;x \sim P(x), P(x) = \phi^x(1-\phi)^{(1-x)}&lt;/script&gt; ，其中 &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;  给出了随机变量 &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; 等于1的概率&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;连续性变量&quot;&gt;连续性变量&lt;/h3&gt;
&lt;p&gt;可以用&lt;strong&gt;概率密码函数(probability density function)&lt;/strong&gt;刻画。&lt;/p&gt;

&lt;h3 id=&quot;常用概率分布&quot;&gt;常用概率分布&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;正态分布(normal distribution)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;  $$\mathcal{N}(x; \mu, \sigma^2) = \sqrt{\frac{1}{2\pi\sigma^2}}\text{exp}(- \frac{1}{2\sigma^2}(x - \mu)^2)$$  &lt;/p&gt;
&lt;p&gt;ND由两个参数决定  &lt;script type=&quot;math/tex&quot;&gt;\mu \in \mathrm{R}, \sigma \in (0, \infty)&lt;/script&gt; , 前者给出中心峰值的座标，即分布的均值； 后者是分布的标准差，控制分布的宽度。&lt;/p&gt;

&lt;p&gt;在缺乏关于某个实数上分布的先验知识时，正态分布是一个比较好的选择。&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;中心极限定理表明很多独立随机变量的和近似服从正态分布。&lt;/li&gt;
  &lt;li&gt;在具有相同方差的所有可能的概率分布中，正态分布在实数上具有最大的不确定性。即正态分布是对模型加入的先验知识量最少的分布。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;指数分布与Laplace分布&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;  $$p(x; \lambda) =  \lambda 1_{x\geq 0} \text{exp}(-\lambda x)$$ &lt;/p&gt;
&lt;p&gt;指数函数  &lt;script type=&quot;math/tex&quot;&gt;1_{x \geq 0} = 0 \text{ if } x \le 0 \text{ else } 1&lt;/script&gt; .&lt;/p&gt;

&lt;p&gt;Laplace分布允许在任意一点  &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt;  设置概率质量函数的峰值：&lt;/p&gt;
&lt;p&gt; $$\text{Laplace}(x; \mu, \gamma) = \frac{1}{2r} \text{exp} (- \frac{\lfloor x - \mu \rfloor}{\gamma})$$ &lt;/p&gt;

&lt;p&gt;To read p103&lt;/p&gt;

&lt;h1 id=&quot;machine-learning-basics&quot;&gt;Machine Learning basics&lt;/h1&gt;
&lt;p&gt;ML本质上属于&lt;strong&gt;应用统计学&lt;/strong&gt;，更多的关注于如何用计算机统计的估算复杂函数，而不太关注为这些函数提供置信空间。&lt;/p&gt;

&lt;p&gt;P 145&lt;/p&gt;

&lt;h1 id=&quot;深度前馈网络deep-feedforward-network&quot;&gt;深度前馈网络(Deep Feedforward Network)&lt;/h1&gt;
&lt;p&gt;也称前馈神经网络，或多层感知机(multilayer perceptron, MLP)。 其目的是近似某个函数  &lt;script type=&quot;math/tex&quot;&gt;f^*&lt;/script&gt;。直观上，FNN可以理解为实现统计泛化而设计出的函数近似机。&lt;/p&gt;

&lt;p&gt;模型称为前向的，因为信息流过&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;的函数，流经用于定义&lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;的中间计算过程，最终到达输出&lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;。在模型的输出和模型本身之间没有反馈(feedback)连接。 当前馈神经网络被扩展包含反馈连接时，它们被称为循环神经网络(recurrent neural network)&lt;/p&gt;

&lt;h3 id=&quot;代价函数&quot;&gt;代价函数&lt;/h3&gt;

&lt;p&gt;贯穿神经网络设计的一个主题是代价函数的梯度必须足够大和具有足够的预测性，来为学习算法提供一个好的指引。&lt;/p&gt;

&lt;h3 id=&quot;线性函数不能表示xor函数&quot;&gt;线性函数不能表示XOR函数&lt;/h3&gt;
&lt;p&gt;XOR函数是两个二进制&lt;script type=&quot;math/tex&quot;&gt;x_1, x_2&lt;/script&gt;的运算。假设存在一个可以表示XOR的线性模型&lt;script type=&quot;math/tex&quot;&gt;\mathcal{M}&lt;/script&gt;, 当&lt;script type=&quot;math/tex&quot;&gt;x_1=0&lt;/script&gt;时，模型随着&lt;script type=&quot;math/tex&quot;&gt;x_2&lt;/script&gt;的增大而增大。当&lt;script type=&quot;math/tex&quot;&gt;x_1=1&lt;/script&gt;时，模型的输出随着&lt;script type=&quot;math/tex&quot;&gt;x_2&lt;/script&gt;的增大而减小。因为线性模型关于变量的系数是固定的，特别的，假设其关于&lt;script type=&quot;math/tex&quot;&gt;x_2&lt;/script&gt;的系数为&lt;script type=&quot;math/tex&quot;&gt;w_2&lt;/script&gt;。由上面讨论可知，&lt;script type=&quot;math/tex&quot;&gt;w_2&lt;/script&gt;依赖于&lt;script type=&quot;math/tex&quot;&gt;x_1&lt;/script&gt;，因此不可能固定，矛盾。&lt;/p&gt;</content><author><name></name></author><category term="Deep Learning" /><category term="AI" /><summary type="html">20世纪40~60年代，控制论(cybernetics)。随着生物学习理论的发展与第一个模型的实现(如感知机1958)，能实现单个神经元的训练。</summary></entry></feed>